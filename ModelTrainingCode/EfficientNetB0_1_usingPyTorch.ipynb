{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e29af1-9371-451b-9386-ad9f91ef878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficientnet_phase1.py\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fcc6570-60fb-4112-b81d-35d9ee349ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure a consistent random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- CONFIG ---\n",
    "BASE_PATH = r\"C:\\Users\\ADITYA DAS\\Desktop\\Machine Learning\\CP_DATASET\"\n",
    "CLASSES = [\"BLIGHT\", \"BLAST\", \"BROWNSPOT\", \"HEALTHY\"]\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-3 # Often higher for training new layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7da854c3-910a-411e-88d1-612f32ea6e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f320906d-17e8-45d2-bfd3-ac5f7f4b55a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom Dataset ---\n",
    "class PlantDiseaseDataset(Dataset):\n",
    "    def __init__(self, filepaths, labels, transform=None, augment=False):\n",
    "        self.filepaths = filepaths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.filepaths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.augment:\n",
    "            # Apply Color Jitter (as part of transform) and GridMask\n",
    "            image = np.array(image) # Convert to numpy for GridMask\n",
    "            image = grid_mask(image)\n",
    "            image = Image.fromarray(image) # Convert back to PIL for torchvision transforms\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6157c0cb-e177-4bd0-a6ac-201887bb1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Augmentation Functions ---\n",
    "def color_jitter_transform():\n",
    "    return transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05)\n",
    "\n",
    "def grid_mask(img, d_min=50, d_max=100, ratio=0.5):\n",
    "    h, w, _ = img.shape\n",
    "    d = random.randint(d_min, d_max)\n",
    "    l = int(d * ratio)\n",
    "\n",
    "    mask = np.ones((h, w), dtype=np.float32)\n",
    "\n",
    "    for i in range(0, h, d):\n",
    "        for j in range(0, w, d):\n",
    "            y1 = i\n",
    "            y2 = min(i + l, h)\n",
    "            x1 = j\n",
    "            x2 = min(j + l, w)\n",
    "\n",
    "            mask[y1:y2, x1:x2] = 0.0\n",
    "\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    return img * mask\n",
    "\n",
    "def cutmix(images, labels, alpha=1.0):\n",
    "    batch_size = images.shape[0]\n",
    "    img_h, img_w = images.shape[2], images.shape[3]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha, size=batch_size)\n",
    "    rand_idx = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_images = images.clone()\n",
    "    mixed_labels = labels.clone()\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        curr_lam = lam[i]\n",
    "        \n",
    "        # Calculate bounding box for cut-and-paste\n",
    "        cut_rat = np.sqrt(1. - curr_lam)\n",
    "        cut_w = img_w * cut_rat\n",
    "        cut_h = img_h * cut_rat\n",
    "\n",
    "        cx = np.random.uniform(0, img_w)\n",
    "        cy = np.random.uniform(0, img_h)\n",
    "\n",
    "        x1 = int(cx - cut_w / 2)\n",
    "        y1 = int(cy - cut_h / 2)\n",
    "        x2 = int(cx + cut_w / 2)\n",
    "        y2 = int(cy + cut_h / 2)\n",
    "\n",
    "        x1 = np.clip(x1, 0, img_w)\n",
    "        y1 = np.clip(y1, 0, img_h)\n",
    "        x2 = np.clip(x2, 0, img_w)\n",
    "        y2 = np.clip(y2, 0, img_h)\n",
    "        \n",
    "        # Adjust lambda based on actual patch size\n",
    "        bb_area = (x2 - x1) * (y2 - y1)\n",
    "        lam_adjusted = 1.0 - (bb_area / (img_w * img_h))\n",
    "\n",
    "        mixed_images[i, :, y1:y2, x1:x2] = images[rand_idx[i], :, y1:y2, x1:x2]\n",
    "\n",
    "        # One-hot encode labels for mixing\n",
    "        label1_onehot = F.one_hot(labels[i], num_classes=len(CLASSES)).float()\n",
    "        label2_onehot = F.one_hot(labels[rand_idx[i]], num_classes=len(CLASSES)).float()\n",
    "        \n",
    "        mixed_labels[i] = lam_adjusted * label1_onehot + (1.0 - lam_adjusted) * label2_onehot\n",
    "    \n",
    "    return mixed_images, mixed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c8a8fd5-13ec-4883-85a5-f8f0a8263b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total images found: 24004\n"
     ]
    }
   ],
   "source": [
    "# --- Load filepaths & labels ---\n",
    "all_filepaths, all_labels = [], []\n",
    "for idx, class_name in enumerate(CLASSES):\n",
    "    aug_path = os.path.join(BASE_PATH, class_name, \"augmented\")\n",
    "    files = glob.glob(os.path.join(aug_path, \"*.jpg\")) + \\\n",
    "            glob.glob(os.path.join(aug_path, \"*.jpeg\")) + \\\n",
    "            glob.glob(os.path.join(aug_path, \"*.png\"))\n",
    "    all_filepaths.extend(files)\n",
    "    all_labels.extend([idx] * len(files))\n",
    "\n",
    "print(f\"âœ… Total images found: {len(all_filepaths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d090e54-034c-41c3-8314-adce2911bd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train samples: 19203 | Val samples: 4801\n"
     ]
    }
   ],
   "source": [
    "# --- Split data ---\n",
    "train_filepaths, val_filepaths, train_labels, val_labels = train_test_split(\n",
    "    all_filepaths, all_labels, test_size=0.2, random_state=SEED, stratify=all_labels\n",
    ")\n",
    "\n",
    "print(f\"âœ… Train samples: {len(train_filepaths)} | Val samples: {len(val_filepaths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e4527df-0f9e-4b24-899d-3c987764c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Transforms (including EfficientNet specific preprocessing) ---\n",
    "# EfficientNet models expect normalization with mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]\n",
    "train_transform = transforms.Compose([\n",
    "    color_jitter_transform(), # Apply color jitter\n",
    "    transforms.ToTensor(), # Converts PIL Image to Tensor and scales to [0, 1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42f8109a-38b8-473e-8409-6cf2858ad96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Datasets and DataLoaders ---\n",
    "# Apply GridMask inside the dataset's __getitem__ when augment=True\n",
    "train_dataset = PlantDiseaseDataset(train_filepaths, train_labels, transform=train_transform, augment=True)\n",
    "val_dataset = PlantDiseaseDataset(val_filepaths, val_labels, transform=val_transform, augment=False) # No GridMask/CutMix on validation\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a864bd23-4e8d-4f30-a3c7-0d17942babb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EfficientNetB0 Model ---\n",
    "import torchvision.models as models\n",
    "\n",
    "class EfficientNetB0_Phase1(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EfficientNetB0_Phase1, self).__init__()\n",
    "        self.backbone = models.efficientnet_b0(weights='IMAGENET1K_V1') # Load pre-trained weights\n",
    "\n",
    "        # Freeze all parameters in the backbone\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Replace the classifier head\n",
    "        num_ftrs = self.backbone.classifier[1].in_features\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes) # No softmax here, CrossEntropyLoss will apply it\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone.features(x)\n",
    "        x = self.backbone.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier_head(x)\n",
    "        return x\n",
    "\n",
    "model = EfficientNetB0_Phase1(len(CLASSES)).to(DEVICE)\n",
    "\n",
    "# --- Loss Function and Optimizer ---\n",
    "# Using CrossEntropyLoss with label_smoothing\n",
    "# PyTorch's CrossEntropyLoss expects raw logits and applies softmax internally.\n",
    "# For label smoothing, we can implement it manually or use a custom loss if not directly available.\n",
    "# A common way to do label smoothing with CrossEntropyLoss is to modify the target labels\n",
    "# Or use a dedicated function if available (e.g., F.one_hot then custom smoothing).\n",
    "# For simplicity, we'll manually apply smoothing during training for the target label in CutMix.\n",
    "# For standard CrossEntropyLoss, it implicitly handles one-hot (after conversion to class index)\n",
    "# For the case of CutMix where labels are fractional, we will use KLDivLoss.\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1) # label_smoothing directly in CrossEntropyLoss for PyTorch >= 1.10\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8faadfc-667d-4403-b22e-6efece75ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Learning Rate Logger ---\n",
    "class LearningRateLogger:\n",
    "    def __init__(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def on_epoch_end(self, epoch):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            lr = param_group['lr']\n",
    "            print(f\"ðŸ“‰ Learning rate at epoch {epoch+1}: {lr:.6f}\")\n",
    "\n",
    "lr_logger = LearningRateLogger(optimizer)\n",
    "\n",
    "# --- Compute class weights ---\n",
    "# Collect all train labels to compute weights\n",
    "train_labels_for_weights = []\n",
    "for _, labels in train_loader:\n",
    "    train_labels_for_weights.extend(labels.cpu().numpy())\n",
    "\n",
    "class_weights_array = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(len(CLASSES)),\n",
    "    y=train_labels_for_weights\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights_array, dtype=torch.float).to(DEVICE)\n",
    "print(\"âœ… Computed class weights:\", class_weights_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77766881-92bd-4482-9b89-18011bc6868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "best_val_accuracy = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
    "    for i, (inputs, labels) in enumerate(train_loader_tqdm):\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        # Apply CutMix\n",
    "        inputs, mixed_labels = cutmix(inputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # If CutMix is applied, labels are one-hot encoded and fractional.\n",
    "        # Use KLDivLoss for such cases, which expects log-softmax outputs.\n",
    "        # Otherwise, use CrossEntropyLoss for integer labels.\n",
    "        if mixed_labels.dim() > 1 and mixed_labels.shape[1] > 1: # Indicates one-hot/fractional labels from CutMix\n",
    "            log_softmax_outputs = F.log_softmax(outputs, dim=1)\n",
    "            loss = F.kl_div(log_softmax_outputs, mixed_labels, reduction='batchmean')\n",
    "        else:\n",
    "            # If not CutMix (e.g., direct labels in val_loader, or if cutmix was skipped)\n",
    "            # Use original CrossEntropyLoss with class weights.\n",
    "            loss = criterion(outputs, labels) # Labels here are original integer labels\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # For accuracy calculation on original labels (before CutMix mixing)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item() # Compares with original integer labels\n",
    "\n",
    "        train_loader_tqdm.set_postfix(loss=running_loss/total_train, acc=correct_train/total_train)\n",
    "\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = correct_train / total_train\n",
    "    print(f\"Epoch {epoch+1} Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    y_true_val, y_pred_val = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\")\n",
    "        for inputs, labels in val_loader_tqdm:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels) # Original labels, no CutMix on validation\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "            y_true_val.extend(labels.cpu().numpy())\n",
    "            y_pred_val.extend(predicted.cpu().numpy())\n",
    "\n",
    "            val_loader_tqdm.set_postfix(loss=val_loss/total_val, acc=correct_val/total_val)\n",
    "\n",
    "\n",
    "    epoch_val_loss = val_loss / len(val_dataset)\n",
    "    epoch_val_acc = correct_val / total_val\n",
    "    print(f\"Epoch {epoch+1} Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "    lr_logger.on_epoch_end(epoch)\n",
    "\n",
    "    # Early Stopping\n",
    "    if epoch_val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = epoch_val_acc\n",
    "        patience_counter = 0\n",
    "        SAVE_PATH = r\"C:\\Users\\ADITYA DAS\\Desktop\\Machine Learning\\CP_MODEL\\EfficientNetB0_Phase1_CutMix_GridMask.pth\"\n",
    "        torch.save(model.state_dict(), SAVE_PATH)\n",
    "        print(f\"âœ… Model saved at: {SAVE_PATH} (Best validation accuracy: {best_val_accuracy:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Patience: {patience_counter}/{4}\")\n",
    "        if patience_counter >= 4:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4424e86e-cee0-46b3-8f04-74d200bd0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation ---\n",
    "print(\"\\nðŸ“Š Final Evaluation on Validation Set:\")\n",
    "model.eval()\n",
    "y_true_final, y_pred_final = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true_final.extend(labels.cpu().numpy())\n",
    "        y_pred_final.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(classification_report(y_true_final, y_pred_final, target_names=CLASSES))\n",
    "\n",
    "cm = confusion_matrix(y_true_final, y_pred_final)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
