{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bda8136-2449-4522-b6a9-a70cbe244487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total images found: 24004\n",
      "✅ Train samples: 19203 | Val samples: 4801\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"add_24\" (type Add).\n\nA merge layer should be called on a list of inputs. Received: inputs=Tensor(\"Placeholder:0\", shape=(None, 56, 56, 128), dtype=float32) (not a list of tensors)\n\nCall arguments received by layer \"add_24\" (type Add):\n  • inputs=tf.Tensor(shape=(None, 56, 56, 128), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 269\u001b[0m\n\u001b[0;32m    267\u001b[0m x \u001b[38;5;241m=\u001b[39m base_model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](inputs) \u001b[38;5;66;03m# Input layer\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m base_model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]: \u001b[38;5;66;03m# All layers except last few classification layers\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m512\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n\u001b[0;32m    272\u001b[0m x \u001b[38;5;241m=\u001b[39m Dropout(\u001b[38;5;241m0.5\u001b[39m)(x)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf2.10.1\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf2.10.1\\lib\\site-packages\\keras\\layers\\merging\\base_merge.py:123\u001b[0m, in \u001b[0;36m_Merge.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 123\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA merge layer should be called on a list of inputs. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: inputs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (not a list of tensors)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m         )\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reshape_required:\n\u001b[0;32m    128\u001b[0m         reshaped_inputs \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"add_24\" (type Add).\n\nA merge layer should be called on a list of inputs. Received: inputs=Tensor(\"Placeholder:0\", shape=(None, 56, 56, 128), dtype=float32) (not a list of tensors)\n\nCall arguments received by layer \"add_24\" (type Add):\n  • inputs=tf.Tensor(shape=(None, 56, 56, 128), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Add, ReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils import class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === CONFIG ===\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BASE_PATH = r\"C:\\Users\\ADITYA DAS\\Desktop\\Machine Learning\\CP_DATASET\"\n",
    "CLASSES = [\"BLIGHT\", \"BLAST\", \"BROWNSPOT\", \"HEALTHY\"]\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-3 # Adjusted learning rate for custom backbone, may need tuning\n",
    "\n",
    "# === SqueezeNext Building Blocks ===\n",
    "def fire_module(x, squeeze_channels, expand1x1_channels, expand3x3_channels):\n",
    "    \"\"\"\n",
    "    SqueezeNet's Fire Module.\n",
    "    \"\"\"\n",
    "    input_channels = x.shape[-1]\n",
    "\n",
    "    # Squeeze Layer\n",
    "    squeezed = Conv2D(squeeze_channels, (1, 1), activation='relu', padding='same')(x)\n",
    "\n",
    "    # Expand Layers\n",
    "    expand1x1 = Conv2D(expand1x1_channels, (1, 1), activation='relu', padding='same')(squeezed)\n",
    "    expand3x3 = Conv2D(expand3x3_channels, (3, 3), activation='relu', padding='same')(squeezed)\n",
    "    return concatenate([expand1x1, expand3x3], axis=-1)\n",
    "\n",
    "def squeeze_next_block(x, out_channels, stride=1, type=\"block_A\"):\n",
    "    \"\"\"\n",
    "    SqueezeNext Block.\n",
    "    \"\"\"\n",
    "    input_channels = x.shape[-1]\n",
    "    \n",
    "    # Bottleneck\n",
    "    bottleneck = Conv2D(out_channels // 2, (1, 1), padding='same', use_bias=False)(x)\n",
    "    bottleneck = BatchNormalization()(bottleneck)\n",
    "    bottleneck = ReLU()(bottleneck)\n",
    "\n",
    "    if type == \"block_A\":\n",
    "        # Grouped 3x3 Conv\n",
    "        conv_3x3 = Conv2D(out_channels // 2, (3, 3), strides=stride, padding='same', groups=out_channels // 2, use_bias=False)(bottleneck)\n",
    "        conv_3x3 = BatchNormalization()(conv_3x3)\n",
    "        conv_3x3 = ReLU()(conv_3x3)\n",
    "    elif type == \"block_B\":\n",
    "        # Grouped 1x3 and 3x1 Convs\n",
    "        conv_1x3 = Conv2D(out_channels // 2, (1, 3), strides=(1, stride), padding='same', groups=out_channels // 2, use_bias=False)(bottleneck)\n",
    "        conv_1x3 = BatchNormalization()(conv_1x3)\n",
    "        conv_1x3 = ReLU()(conv_1x3)\n",
    "        conv_3x1 = Conv2D(out_channels // 2, (3, 1), strides=(stride, 1), padding='same', groups=out_channels // 2, use_bias=False)(conv_1x3)\n",
    "        conv_3x1 = BatchNormalization()(conv_3x1)\n",
    "        conv_3x1 = ReLU()(conv_3x1)\n",
    "        conv_3x3 = conv_3x1 # Renamed for consistency\n",
    "\n",
    "    # Expansion\n",
    "    expanded = Conv2D(out_channels, (1, 1), padding='same', use_bias=False)(conv_3x3)\n",
    "    expanded = BatchNormalization()(expanded)\n",
    "\n",
    "    # Shortcut\n",
    "    if stride != 1 or input_channels != out_channels:\n",
    "        shortcut = Conv2D(out_channels, (1, 1), strides=stride, padding='same', use_bias=False)(x)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    else:\n",
    "        shortcut = x\n",
    "    \n",
    "    return ReLU()(Add()([expanded, shortcut]))\n",
    "\n",
    "\n",
    "def SqueezeNext(input_shape=(224, 224, 3), num_classes=1000, version=\"1.0\"):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), strides=(2, 2), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    if version == \"1.0\": # SqueezeNext-50, 1.0 (from paper)\n",
    "        x = squeeze_next_block(x, 128, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 128, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 256, stride=2, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 256, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 512, stride=2, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 512, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 1024, stride=2, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 1024, type=\"block_A\")\n",
    "    elif version == \"2.0\": # SqueezeNext-50, 2.0 (using 1x3 and 3x1 for some blocks)\n",
    "        x = squeeze_next_block(x, 128, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 128, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 256, stride=2, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 256, type=\"block_B\") # Change here\n",
    "        x = squeeze_next_block(x, 512, stride=2, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 512, type=\"block_B\") # Change here\n",
    "        x = squeeze_next_block(x, 1024, stride=2, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 1024, type=\"block_B\") # Change here\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# === Load filepaths & labels ===\n",
    "all_filepaths, all_labels = [], []\n",
    "for idx, class_name in enumerate(CLASSES):\n",
    "    aug_path = os.path.join(BASE_PATH, class_name, \"augmented\")\n",
    "    files = glob.glob(os.path.join(aug_path, \"*.jpg\")) + \\\n",
    "            glob.glob(os.path.join(aug_path, \"*.jpeg\")) + \\\n",
    "            glob.glob(os.path.join(aug_path, \"*.png\"))\n",
    "    all_filepaths.extend(files)\n",
    "    all_labels.extend([idx] * len(files))\n",
    "\n",
    "print(f\"✅ Total images found: {len(all_filepaths)}\")\n",
    "\n",
    "# === tf.data.Dataset ===\n",
    "filepaths_ds = tf.data.Dataset.from_tensor_slices(all_filepaths)\n",
    "labels_ds = tf.data.Dataset.from_tensor_slices(all_labels)\n",
    "ds = tf.data.Dataset.zip((filepaths_ds, labels_ds)).shuffle(len(all_filepaths), seed=SEED)\n",
    "\n",
    "train_size = int(0.8 * len(all_filepaths))\n",
    "train_ds = ds.take(train_size)\n",
    "val_ds = ds.skip(train_size)\n",
    "\n",
    "print(f\"✅ Train samples: {train_size} | Val samples: {len(all_filepaths) - train_size}\")\n",
    "\n",
    "# === Color Jitter ===\n",
    "def color_jitter(image):\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
    "    image = tf.image.random_hue(image, max_delta=0.05)\n",
    "    return tf.clip_by_value(image, 0.0, 255.0)\n",
    "\n",
    "# === GridMask ===\n",
    "def grid_mask(image, d_min=50, d_max=100, ratio=0.5):\n",
    "    h, w, _ = image.shape\n",
    "    d = tf.random.uniform([], d_min, d_max, dtype=tf.int32)\n",
    "    l = tf.cast(tf.cast(d, tf.float32) * ratio, tf.int32)\n",
    "\n",
    "    mask = tf.ones([h, w], dtype=tf.float32)\n",
    "\n",
    "    for i in range(0, h, d):\n",
    "        for j in range(0, w, d):\n",
    "            y1 = i\n",
    "            y2 = tf.minimum(i + l, h)\n",
    "            x1 = j\n",
    "            x2 = tf.minimum(j + l, w)\n",
    "\n",
    "            y_range = tf.range(y1, y2)\n",
    "            x_range = tf.range(x1, x2)\n",
    "            yy, xx = tf.meshgrid(y_range, x_range, indexing='ij')\n",
    "            indices = tf.stack([yy, xx], axis=-1)\n",
    "            indices = tf.reshape(indices, [-1, 2])\n",
    "\n",
    "            mask = tf.tensor_scatter_nd_update(\n",
    "                mask,\n",
    "                indices,\n",
    "                tf.zeros([(y2 - y1) * (x2 - x1)], dtype=tf.float32)\n",
    "            )\n",
    "\n",
    "    mask = tf.expand_dims(mask, axis=-1)\n",
    "    mask = tf.tile(mask, [1, 1, 3])\n",
    "    return image * mask\n",
    "\n",
    "# === Image Processor ===\n",
    "def preprocess_for_squeezenext(image):\n",
    "    # SqueezeNext typically expects input in range [0, 1] or normalized.\n",
    "    # We'll normalize to [0, 1]\n",
    "    return image / 255.0\n",
    "\n",
    "def process_img(filepath, label):\n",
    "    img = tf.io.read_file(filepath)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "\n",
    "    img = color_jitter(img)\n",
    "    img = grid_mask(img)\n",
    "\n",
    "    img = preprocess_for_squeezenext(img) # Custom preprocessing for SqueezeNext\n",
    "\n",
    "    label = tf.one_hot(label, depth=len(CLASSES))\n",
    "    return img, label\n",
    "\n",
    "# === CutMix ===\n",
    "def cutmix(images, labels, alpha=1.0):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    img_h = tf.shape(images)[1]\n",
    "    img_w = tf.shape(images)[2]\n",
    "\n",
    "    lam = tfp.distributions.Beta(alpha, alpha).sample([batch_size])\n",
    "\n",
    "    rand_idx = tf.random.shuffle(tf.range(batch_size))\n",
    "    images2 = tf.gather(images, rand_idx)\n",
    "    labels2 = tf.gather(labels, rand_idx)\n",
    "\n",
    "    cut_rat = tf.math.sqrt(1. - lam)\n",
    "    cut_w = tf.cast(img_w, tf.float32) * cut_rat\n",
    "    cut_h = tf.cast(img_h, tf.float32) * cut_rat\n",
    "\n",
    "    cx = tf.random.uniform([batch_size], 0, tf.cast(img_w, tf.float32))\n",
    "    cy = tf.random.uniform([batch_size], 0, tf.cast(img_h, tf.float32))\n",
    "\n",
    "    x1 = tf.cast(cx - cut_w / 2, tf.int32)\n",
    "    y1 = tf.cast(cy - cut_h / 2, tf.int32)\n",
    "    x2 = tf.cast(cx + cut_w / 2, tf.int32)\n",
    "    y2 = tf.cast(cy + cut_h / 2, tf.int32)\n",
    "\n",
    "    x1 = tf.clip_by_value(x1, 0, img_w)\n",
    "    y1 = tf.clip_by_value(y1, 0, img_h)\n",
    "    x2 = tf.clip_by_value(x2, 0, img_w)\n",
    "    y2 = tf.clip_by_value(y2, 0, img_h)\n",
    "\n",
    "    def apply_cutmix(i):\n",
    "        img1 = images[i]\n",
    "        img2 = images2[i]\n",
    "        bbx1, bby1, bbx2, bby2 = x1[i], y1[i], x2[i], y2[i]\n",
    "\n",
    "        mask = tf.pad(\n",
    "            tf.zeros([bby2 - bby1, bbx2 - bbx1, 3]),\n",
    "            [[bby1, img_h - bby2],\n",
    "             [bbx1, img_w - bbx2],\n",
    "             [0, 0]],\n",
    "            constant_values=1.0\n",
    "        )\n",
    "        mask = 1.0 - mask\n",
    "        mixed = img1 * mask + img2 * (1.0 - mask)\n",
    "\n",
    "        area = tf.cast(bbx2 - bbx1, tf.float32) * tf.cast(bby2 - bby1, tf.float32)\n",
    "        lam_adjusted = 1.0 - (area / tf.cast(img_w * img_h, tf.float32))\n",
    "        new_label = lam_adjusted * labels[i] + (1.0 - lam_adjusted) * labels2[i]\n",
    "\n",
    "        return mixed, new_label\n",
    "\n",
    "    mixed_images, mixed_labels = tf.map_fn(\n",
    "        apply_cutmix,\n",
    "        tf.range(batch_size),\n",
    "        fn_output_signature=(tf.float32, tf.float32)\n",
    "    )\n",
    "\n",
    "    return mixed_images, mixed_labels\n",
    "\n",
    "# === Final Pipeline ===\n",
    "train_ds = train_ds.map(process_img).batch(BATCH_SIZE)\n",
    "train_ds = train_ds.map(lambda x, y: cutmix(x, y)).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(process_img).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# === SqueezeNext Model ===\n",
    "base_model = SqueezeNext(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), num_classes=len(CLASSES), version=\"1.0\")\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers[:-4]: # Adjust this based on where you want to cut off\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom classification head on top of the frozen base\n",
    "inputs = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "x = base_model.layers[0](inputs) # Input layer\n",
    "for layer in base_model.layers[1:-3]: # All layers except last few classification layers\n",
    "    x = layer(x)\n",
    "\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "outputs = Dense(len(CLASSES), activation='softmax')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# === Learning rate logger ===\n",
    "class LearningRateLogger(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        if hasattr(lr, '__call__'):\n",
    "            lr = lr(self.model.optimizer.iterations)\n",
    "        if hasattr(lr, 'numpy'):\n",
    "            lr = lr.numpy()\n",
    "        print(f\"📉 Learning rate at epoch {epoch+1}: {lr:.6f}\")\n",
    "\n",
    "# === Compute class weights ===\n",
    "y_train_int = np.argmax(np.concatenate([labels.numpy() for _, labels in train_ds.unbatch().batch(BATCH_SIZE)]), axis=1)\n",
    "class_weights = dict(enumerate(class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(len(CLASSES)),\n",
    "    y=y_train_int\n",
    ")))\n",
    "print(\"✅ Computed class weights:\", class_weights)\n",
    "\n",
    "# === Train ===\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[EarlyStopping(patience=4, restore_best_weights=True), LearningRateLogger()],\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# === Evaluate ===\n",
    "y_true, y_pred = [], []\n",
    "for images, labels in val_ds:\n",
    "    preds = model.predict(images)\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=CLASSES))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "ConfusionMatrixDisplay(cm, display_labels=CLASSES).plot(cmap=\"Blues\", xticks_rotation=45)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Save ===\n",
    "SAVE_PATH = r\"C:\\Users\\ADITYA DAS\\Desktop\\Machine Learning\\CP_MODEL\\SqueezeNext_Phase1_CutMix_GridMask.h5\"\n",
    "model.save(SAVE_PATH)\n",
    "print(f\"✅ Model saved at: {SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
