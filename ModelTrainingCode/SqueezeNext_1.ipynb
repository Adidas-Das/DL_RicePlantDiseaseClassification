{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65a27825-fd78-43e5-8b0f-ee5b28f815f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Add, ReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils import class_weight\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74af793-4bdf-46ef-9526-3412541f862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BASE_PATH = r\"C:\\Users\\ADITYA DAS\\Desktop\\Machine Learning\\CP_DATASET\"\n",
    "CLASSES = [\"BLIGHT\", \"BLAST\", \"BROWNSPOT\", \"HEALTHY\"]\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-3 # Adjusted learning rate for custom backbone, may need tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d85850-6f1a-4ee4-8107-af0413d5e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SqueezeNext Building Blocks ===\n",
    "def fire_module(x, squeeze_channels, expand1x1_channels, expand3x3_channels):\n",
    "    \"\"\"\n",
    "    SqueezeNet's Fire Module.\n",
    "    \"\"\"\n",
    "    input_channels = x.shape[-1]\n",
    "\n",
    "    # Squeeze Layer\n",
    "    squeezed = Conv2D(squeeze_channels, (1, 1), activation='relu', padding='same')(x)\n",
    "\n",
    "    # Expand Layers\n",
    "    expand1x1 = Conv2D(expand1x1_channels, (1, 1), activation='relu', padding='same')(squeezed)\n",
    "    expand3x3 = Conv2D(expand3x3_channels, (3, 3), activation='relu', padding='same')(squeezed)\n",
    "    return concatenate([expand1x1, expand3x3], axis=-1)\n",
    "\n",
    "def squeeze_next_block(x, out_channels, stride=1, type=\"block_A\"):\n",
    "    \"\"\"\n",
    "    SqueezeNext Block.\n",
    "    \"\"\"\n",
    "    input_channels = x.shape[-1]\n",
    "    \n",
    "    # Bottleneck\n",
    "    bottleneck = Conv2D(out_channels // 2, (1, 1), padding='same', use_bias=False)(x)\n",
    "    bottleneck = BatchNormalization()(bottleneck)\n",
    "    bottleneck = ReLU()(bottleneck)\n",
    "\n",
    "    if type == \"block_A\":\n",
    "        # Grouped 3x3 Conv\n",
    "        conv_3x3 = Conv2D(out_channels // 2, (3, 3), strides=stride, padding='same', groups=out_channels // 2, use_bias=False)(bottleneck)\n",
    "        conv_3x3 = BatchNormalization()(conv_3x3)\n",
    "        conv_3x3 = ReLU()(conv_3x3)\n",
    "    elif type == \"block_B\":\n",
    "        # Grouped 1x3 and 3x1 Convs\n",
    "        conv_1x3 = Conv2D(out_channels // 2, (1, 3), strides=(1, stride), padding='same', groups=out_channels // 2, use_bias=False)(bottleneck)\n",
    "        conv_1x3 = BatchNormalization()(conv_1x3)\n",
    "        conv_1x3 = ReLU()(conv_1x3)\n",
    "        conv_3x1 = Conv2D(out_channels // 2, (3, 1), strides=(stride, 1), padding='same', groups=out_channels // 2, use_bias=False)(conv_1x3)\n",
    "        conv_3x1 = BatchNormalization()(conv_3x1)\n",
    "        conv_3x1 = ReLU()(conv_3x1)\n",
    "        conv_3x3 = conv_3x1 # Renamed for consistency\n",
    "\n",
    "    # Expansion\n",
    "    expanded = Conv2D(out_channels, (1, 1), padding='same', use_bias=False)(conv_3x3)\n",
    "    expanded = BatchNormalization()(expanded)\n",
    "\n",
    "    # Shortcut\n",
    "    if stride != 1 or input_channels != out_channels:\n",
    "        shortcut = Conv2D(out_channels, (1, 1), strides=stride, padding='same', use_bias=False)(x)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    else:\n",
    "        shortcut = x\n",
    "    \n",
    "    return ReLU()(Add()([expanded, shortcut]))\n",
    "\n",
    "\n",
    "def SqueezeNext(input_shape=(224, 224, 3), num_classes=1000, version=\"1.0\"):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), strides=(2, 2), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    if version == \"1.0\": # SqueezeNext-50, 1.0 (from paper)\n",
    "        x = squeeze_next_block(x, 128, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 128, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 256, stride=2, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 256, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 512, stride=2, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 512, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 1024, stride=2, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 1024, type=\"block_A\")\n",
    "    elif version == \"2.0\": # SqueezeNext-50, 2.0 (using 1x3 and 3x1 for some blocks)\n",
    "        x = squeeze_next_block(x, 128, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 128, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 256, stride=2, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 256, type=\"block_B\") # Change here\n",
    "        x = squeeze_next_block(x, 512, stride=2, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 512, type=\"block_B\") # Change here\n",
    "        x = squeeze_next_block(x, 1024, stride=2, type=\"block_A\")\n",
    "        x = squeeze_next_block(x, 1024, type=\"block_B\") # Change here\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bae757f-078b-4f10-93a5-e3aa1daeb231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total images found: 24004\n"
     ]
    }
   ],
   "source": [
    "# === Load filepaths & labels ===\n",
    "all_filepaths, all_labels = [], []\n",
    "for idx, class_name in enumerate(CLASSES):\n",
    "    aug_path = os.path.join(BASE_PATH, class_name, \"augmented\")\n",
    "    files = glob.glob(os.path.join(aug_path, \"*.jpg\")) + \\\n",
    "            glob.glob(os.path.join(aug_path, \"*.jpeg\")) + \\\n",
    "            glob.glob(os.path.join(aug_path, \"*.png\"))\n",
    "    all_filepaths.extend(files)\n",
    "    all_labels.extend([idx] * len(files))\n",
    "\n",
    "print(f\"✅ Total images found: {len(all_filepaths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "514f4688-9cde-4b4f-8dce-17614fc497c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train samples: 19203 | Val samples: 4801\n"
     ]
    }
   ],
   "source": [
    "# === tf.data.Dataset ===\n",
    "filepaths_ds = tf.data.Dataset.from_tensor_slices(all_filepaths)\n",
    "labels_ds = tf.data.Dataset.from_tensor_slices(all_labels)\n",
    "ds = tf.data.Dataset.zip((filepaths_ds, labels_ds)).shuffle(len(all_filepaths), seed=SEED)\n",
    "\n",
    "train_size = int(0.8 * len(all_filepaths))\n",
    "train_ds = ds.take(train_size)\n",
    "val_ds = ds.skip(train_size)\n",
    "\n",
    "print(f\"✅ Train samples: {train_size} | Val samples: {len(all_filepaths) - train_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f5ec5ae-ec0c-483d-8f4b-d54cf4bf1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Color Jitter ===\n",
    "def color_jitter(image):\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
    "    image = tf.image.random_hue(image, max_delta=0.05)\n",
    "    return tf.clip_by_value(image, 0.0, 255.0)\n",
    "\n",
    "# # === GridMask ===\n",
    "# def grid_mask(image, d_min=50, d_max=100, ratio=0.5):\n",
    "#     h, w, _ = image.shape\n",
    "#     d = tf.random.uniform([], d_min, d_max, dtype=tf.int32)\n",
    "#     l = tf.cast(tf.cast(d, tf.float32) * ratio, tf.int32)\n",
    "\n",
    "#     mask = tf.ones([h, w], dtype=tf.float32)\n",
    "\n",
    "#     for i in range(0, h, d):\n",
    "#         for j in range(0, w, d):\n",
    "#             y1 = i\n",
    "#             y2 = tf.minimum(i + l, h)\n",
    "#             x1 = j\n",
    "#             x2 = tf.minimum(j + l, w)\n",
    "\n",
    "#             y_range = tf.range(y1, y2)\n",
    "#             x_range = tf.range(x1, x2)\n",
    "#             yy, xx = tf.meshgrid(y_range, x_range, indexing='ij')\n",
    "#             indices = tf.stack([yy, xx], axis=-1)\n",
    "#             indices = tf.reshape(indices, [-1, 2])\n",
    "\n",
    "#             mask = tf.tensor_scatter_nd_update(\n",
    "#                 mask,\n",
    "#                 indices,\n",
    "#                 tf.zeros([(y2 - y1) * (x2 - x1)], dtype=tf.float32)\n",
    "#             )\n",
    "\n",
    "#     mask = tf.expand_dims(mask, axis=-1)\n",
    "#     mask = tf.tile(mask, [1, 1, 3])\n",
    "#     return image * mask\n",
    "\n",
    "# === Image Processor ===\n",
    "def preprocess_for_squeezenext(image):\n",
    "    # SqueezeNext typically expects input in range [0, 1] or normalized.\n",
    "    # We'll normalize to [0, 1]\n",
    "    return image / 255.0\n",
    "\n",
    "def process_img(filepath, label):\n",
    "    img = tf.io.read_file(filepath)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "\n",
    "    img = color_jitter(img)\n",
    "    # img = grid_mask(img)\n",
    "\n",
    "    img = preprocess_for_squeezenext(img) # Custom preprocessing for SqueezeNext\n",
    "\n",
    "    label = tf.one_hot(label, depth=len(CLASSES))\n",
    "    return img, label\n",
    "\n",
    "# # === CutMix ===\n",
    "# def cutmix(images, labels, alpha=1.0):\n",
    "#     batch_size = tf.shape(images)[0]\n",
    "#     img_h = tf.shape(images)[1]\n",
    "#     img_w = tf.shape(images)[2]\n",
    "\n",
    "#     lam = tfp.distributions.Beta(alpha, alpha).sample([batch_size])\n",
    "\n",
    "#     rand_idx = tf.random.shuffle(tf.range(batch_size))\n",
    "#     images2 = tf.gather(images, rand_idx)\n",
    "#     labels2 = tf.gather(labels, rand_idx)\n",
    "\n",
    "#     cut_rat = tf.math.sqrt(1. - lam)\n",
    "#     cut_w = tf.cast(img_w, tf.float32) * cut_rat\n",
    "#     cut_h = tf.cast(img_h, tf.float32) * cut_rat\n",
    "\n",
    "#     cx = tf.random.uniform([batch_size], 0, tf.cast(img_w, tf.float32))\n",
    "#     cy = tf.random.uniform([batch_size], 0, tf.cast(img_h, tf.float32))\n",
    "\n",
    "#     x1 = tf.cast(cx - cut_w / 2, tf.int32)\n",
    "#     y1 = tf.cast(cy - cut_h / 2, tf.int32)\n",
    "#     x2 = tf.cast(cx + cut_w / 2, tf.int32)\n",
    "#     y2 = tf.cast(cy + cut_h / 2, tf.int32)\n",
    "\n",
    "#     x1 = tf.clip_by_value(x1, 0, img_w)\n",
    "#     y1 = tf.clip_by_value(y1, 0, img_h)\n",
    "#     x2 = tf.clip_by_value(x2, 0, img_w)\n",
    "#     y2 = tf.clip_by_value(y2, 0, img_h)\n",
    "\n",
    "#     def apply_cutmix(i):\n",
    "#         img1 = images[i]\n",
    "#         img2 = images2[i]\n",
    "#         bbx1, bby1, bbx2, bby2 = x1[i], y1[i], x2[i], y2[i]\n",
    "\n",
    "#         mask = tf.pad(\n",
    "#             tf.zeros([bby2 - bby1, bbx2 - bbx1, 3]),\n",
    "#             [[bby1, img_h - bby2],\n",
    "#              [bbx1, img_w - bbx2],\n",
    "#              [0, 0]],\n",
    "#             constant_values=1.0\n",
    "#         )\n",
    "#         mask = 1.0 - mask\n",
    "#         mixed = img1 * mask + img2 * (1.0 - mask)\n",
    "\n",
    "#         area = tf.cast(bbx2 - bbx1, tf.float32) * tf.cast(bby2 - bby1, tf.float32)\n",
    "#         lam_adjusted = 1.0 - (area / tf.cast(img_w * img_h, tf.float32))\n",
    "#         new_label = lam_adjusted * labels[i] + (1.0 - lam_adjusted) * labels2[i]\n",
    "\n",
    "#         return mixed, new_label\n",
    "\n",
    "#     mixed_images, mixed_labels = tf.map_fn(\n",
    "#         apply_cutmix,\n",
    "#         tf.range(batch_size),\n",
    "#         fn_output_signature=(tf.float32, tf.float32)\n",
    "#     )\n",
    "\n",
    "#     return mixed_images, mixed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6c027ca-80d4-4b36-ad20-538c248b318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Final Pipeline ===\n",
    "train_ds = train_ds.map(process_img).batch(BATCH_SIZE)\n",
    "# train_ds = train_ds.map(lambda x, y: cutmix(x, y)).prefetch(tf.data.AUTOTUNE) # Comment this line out\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE) # Add this line to keep prefetching\n",
    "\n",
    "val_ds = val_ds.map(process_img).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d41bbe7-cc07-4988-aaef-063aa7048494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SqueezeNext Model ===\n",
    "base_model = SqueezeNext(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), num_classes=len(CLASSES), version=\"1.0\")\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    if not isinstance(layer, (GlobalAveragePooling2D, Dense, Dropout)): # Freeze all layers except the last classification ones\n",
    "        layer.trainable = False\n",
    "\n",
    "# Build the new model on top of the frozen base_model\n",
    "inputs = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "x = base_model(inputs, training=False) # Call the base_model directly\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "outputs = Dense(len(CLASSES), activation='softmax')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7a14504-aa30-4252-9ffa-d5045387b77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Computed class weights: {0: 1.6669270833333334, 1: 0.8733400036383482, 2: 1.1053994934377158, 3: 0.7405136510874595}\n"
     ]
    }
   ],
   "source": [
    "# === Learning rate logger ===\n",
    "class LearningRateLogger(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        if hasattr(lr, '__call__'):\n",
    "            lr = lr(self.model.optimizer.iterations)\n",
    "        if hasattr(lr, 'numpy'):\n",
    "            lr = lr.numpy()\n",
    "        print(f\"📉 Learning rate at epoch {epoch+1}: {lr:.6f}\")\n",
    "\n",
    "# === Compute class weights ===\n",
    "y_train_int = np.argmax(np.concatenate([labels.numpy() for _, labels in train_ds.unbatch().batch(BATCH_SIZE)]), axis=1)\n",
    "class_weights = dict(enumerate(class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(len(CLASSES)),\n",
    "    y=y_train_int\n",
    ")))\n",
    "print(\"✅ Computed class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718aa113-14fe-45b4-9b2c-fcdb1d4bf7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Train ===\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[EarlyStopping(patience=4, restore_best_weights=True), LearningRateLogger()],\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c825ac0-55db-400c-aa80-a2192403e622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluate ===\n",
    "y_true, y_pred = [], []\n",
    "for images, labels in val_ds:\n",
    "    preds = model.predict(images)\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=CLASSES))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "ConfusionMatrixDisplay(cm, display_labels=CLASSES).plot(cmap=\"Blues\", xticks_rotation=45)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Save ===\n",
    "SAVE_PATH = r\"C:\\Users\\ADITYA DAS\\Desktop\\Machine Learning\\CP_MODEL\\SqueezeNext_Phase1_CutMix_GridMask.h5\"\n",
    "model.save(SAVE_PATH)\n",
    "print(f\"✅ Model saved at: {SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
